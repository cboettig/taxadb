---
title: "taxadb: A High-Performance Local Taxonomic Database Interface"
author:
  - name: "Kari Norman"
    affiliation: ucb
  - name: "Jorrit Poelen"
    affiliation: jorritt
  - name: "Scott Chamberlain"
    affiliation: ropensci      
  - name: "Carl Boettiger"
    affiliation: ucb, 1
address:
  - code: ucb
    address: "Dept of Environmental Science, Policy, and Management, University of California Berkeley, Berkeley CA 94720-3114, USA"
  - code: jorrit
    address: "Independent consultant, Oakland, CA, USA"
  - code: ropensci
    address: "The rOpenSci Project, University of California Berkeley, Berkeley CA 94720-3114, USA"
abstract: |
  A familiar and growing challenge in ecological and evolutionary research is that of reconciling scientific names of relevant taxa when combining data from separate sources. While this problem is already well understood and numerous naming authorities have been created to address the issue, most researchers lack a fast, consistent, and intuitive way to reconcile taxonomic names.  Here, we present `taxadb` R package to address this gap.  In contrast to existing tools, `taxadb` provides the following:
  1) `taxadb` accesses established naming authorities to resolve synonyms, IDs, and hiearchical classification.  
  2) `taxadb` creates a local database, managed automatically from within R, that provides fast operations on millions of taxonomic names.
  3) `taxadb` provides a consistent and intuitive data format
  4) `taxadb` is built on a simple, extensible and language agnostic design that can easily accomodiate new authorities.
  
journal: "Methods in Ecology & Evolution"
date: "`r Sys.Date()`"
bibliography: refs.bib
output: rticles::elsevier_article
---

```{r include=FALSE}
knitr::opts_chunk$set(message=FALSE)
```

The purpose and strengths of `taxadb` can be most easily seen through an introductory example.  

```{r message=FALSE}
library(tidyverse)
library(taxadb)
```

```{r message=FALSE}
td_create("all")
```


```{r message=FALSE}
bbs <- read_tsv(system.file("extdata/bbs.tsv", package="taxadb"))
dim(bbs)
```


```{r}
bbs_ids <- 
bbs %>% 
  select(species) %>%
  mutate(
         gbif = get_ids(species, "gbif", "prefix"),
          col = get_ids(species, "col",  "prefix"),
         itis = get_ids(species, "itis", "prefix"),
         ncbi = get_ids(species, "ncbi", "prefix"),
     wikidata = get_ids(species, "wd",   "prefix"),
         iucn = get_ids(species, "iucn", "prefix"),
          ott = get_ids(species, "ott",  "prefix") 
  )

```



```{r}
bbs_ids %>% 
  select(-species) %>% 
  purrr::map_dbl(function(x) sum(!is.na(x)))
```

`get_ids` by default looks only for exact matches.  We can get more matches by cleaning the names first.  The `clean_names()` function provides several simple string manipulations to tidy names to improve the probability of a match: missing species (specific eptithets) such as `Accipiter sp.` drop the `sp.`, allowing matches against Genus names, and intraspecific epithets such as `Colaptes auratus cafer` are dropped to binomial names, `Colaptes auratus`.  These transformations may not be appropriate for certain use cases, and should be used with care, and only once as many untransformed names as possible have already been matched. 

In this example, applying this transformation on the unmatched names we can bring matches against OTT identifiers up from 665 to 740, missing only ten names:

```{r}
ott_ids <- bbs %>% 
  mutate(names = clean_names(species)) %>% 
  pull(names) %>% 
  ids("ott")

unmatched <- ott_ids %>% 
  filter(is.na(acceptedNameUsageID)) %>% 
  select(input, sort)
```



```{r}
syn <- unmatched$input %>% synonyms("iucn")
syn
```

Three of these synonyms are considered accepted names in OTT:


```{r}
syn$synonym %>% na.omit() %>% ids("ott")
```


```{r}
syn$synonym %>% na.omit() %>% get_ids("ott")
syn %>% na.omit() %>% mutate(
  #ott = get_ids(synonym, "ott"),
                             col = get_ids(synonym, "col"),
                             itis = get_ids(synonym, "itis"),
                             gbif = get_ids(synonym, "gbif"))
```









```{r}

syns <- unmatched$input %>% ids("iucn") %>%
        dplyr::filter(taxonomicStatus != "accepted") %>%
        dplyr::select(synonym = scientificName,
                      synonym_id = taxonID,
                      taxonomicStatus,
                      acceptedNameUsageID)
accepted


syn <- unmatched$input %>% synonyms("iucn")
syn
```

```{r}

## a name that resolves to multiple possible IDs is not considered a match by `get_ids()`
ott_ids <- bbs %>% 
  mutate(names = clean_names(species)) %>%
  pull(names) %>% 
  get_ids("ott", "prefix")



length(!is.na(ott_ids))
```

```{r}
cleaned <- 
  bbs$species[is.na(bbs_ott)] %>% 
  clean_names() %>% 
  get_ids("ott", "prefix")

## use ID from cleaned name only if no exact match.
bbs_ott[is.na(bbs_ott)] <- cleaned
```

We are still missing 10 species: 

```{r}
still_missing <- bbs$species[is.na(bbs_ott)]
still_missing
```

Three of these are recognizably common names, which we can resolve against the common names table: 




```{r}
clean_names(still_missing) %>% ids("gbif")
```



(GBIF lists *Antigone canadensis* as a synonym for *Grus canadensis*; this appears backwards as the Sandhill Crane was moved out of *Grus* after molecular analyses showed it made that taxon polyphyletic; https://en.wikipedia.org/wiki/Sandhill_crane). 


1. A problem already solved  including ITIS, NCBI, CoL, GBIF, WikiData, FishBase, and The Plant List


2. Performance.  A fundamental challenge of most existing services is the use of internet-based Application Programming Interfaces, (web API)s to resolve individual names.  The motivation for an API-based approach is reasonable: (1) Web APIs are a ubiquitous standard of data exchange (2) make very minimal assumptions about the language, software, or hardware avaialable to the user, (3) users access a single centrally managed database that is maintained and upgraded by the provider, and (4) the provider can easily control access to the data, measure usage statistics, and enforce rate limits.  Many existing tools provide bindings of these APIs as functions to a specific language [@chamberlain2013; @taxize; @itis; @rotl; @worrms; @wikitaxa]

Instead of binding existing web APIs, `taxadb` is built around a set compressed text files following a simple and consistent schema (discussed below). These files are automatically downloaded and imported and stored on a local database by `taxadb`.  By default `taxadb` uses MonetDBLite [@monetdblite], a columnar-oriented relational database requiring no additional installation and providing persistent disk-based storage which can be orders of magnitude faster than traditional database servers such as Postgres [@monetdblite]

3. Consistency 

